# -*- coding: utf-8 -*-
"""heart_disease_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/126r8hC1UJNjjsFb8NSrYSsLah3kvC_eI
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load dataset
heart_factors = pd.read_csv("/content/framingham (1).csv")
heart_factors.head()

import os

os.getcwd()

# getting num of rows and columns in database
heart_factors.shape

#getting info about data
heart_factors.info()

#checking for missing values
heart_factors.isnull().sum()

heart_factors['education'].fillna(heart_factors['education'].median(),inplace=True)
heart_factors['cigsPerDay'].fillna(heart_factors['cigsPerDay'].median(),inplace=True)
heart_factors['BPMeds'].fillna(heart_factors['BPMeds'].median(),inplace=True)
heart_factors['totChol'].fillna(heart_factors['totChol'].median(),inplace=True)
heart_factors['BMI'].fillna(heart_factors['BMI'].median(),inplace=True)
heart_factors['heartRate'].fillna(heart_factors['heartRate'].median(),inplace=True)
heart_factors['glucose'].fillna(heart_factors['glucose'].median(),inplace=True)

heart_factors.isnull().sum()

# statistical measure about the data
heart_factors.describe()

# checking the distribution ofTarget value
heart_factors['TenYearCHD'].value_counts()

# Splitting the Features and Target
X = heart_factors.drop(columns='TenYearCHD',axis=1)
Y = heart_factors['TenYearCHD']
print(X)

#splitting the data into training data & test data
X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=2)

print(X.shape,X_train.shape,X_test.shape)

#model training
#logistic regression
model=LogisticRegression()

#training the logistic regression model withtraining data
model.fit(X_train,Y_train)

LogisticRegression()

#Model Evaluation
#Accuracy score
#accuray on training data
X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction,Y_train)

print('Accuracy on training data : ' ,training_data_accuracy)

# accuracy on test data
X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction,Y_test)

print('Accuracy on test data : ' , test_data_accuracy)

#building a predictive system

import numpy as np
input_data = (43,1,1,30,0,0,1,0,225,162,107,23,61,93,88)

#change the input data to a numpy array
input_data_as_numpy_array = np.asarray(input_data) # Corrected variable name from input_data_as_ndeumpy_array to input_data_as_numpy_array

# reshape the numpy array as we are predicting for only on instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

prediction = model.predict(input_data_reshaped)
print(prediction)

if(prediction[0]==0):
  print('The Person does not have a Heart Disease')
else:
  print('The Person has Heart Diseases')

#decision tree classifier
dc_clf = DecisionTreeClassifier()
dc_clf.fit(X_train,Y_train)

dlf_pred = dc_clf.predict(X_test)
print(accuracy_score(Y_test,dlf_pred))

#Random forest classifier
from sklearn.ensemble import RandomForestClassifier # Import RandomForestClassifier from sklearn.ensemble
rc_clf = RandomForestClassifier()
rc_clf.fit(X_train,Y_train)

Rfc_pred = rc_clf.predict(X_test)
print(accuracy_score(Y_test,Rfc_pred))

#SVM Classifier
svm_clf = SVC()
svm_clf.fit(X_train,Y_train)

svm_pred = rc_clf.predict(X_test)
print(accuracy_score(Y_test , svm_pred))

import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier

# Define knn_score before the loop
knn_score = []
for k in range(1, 21):
    knn_classifier = KNeighborsClassifier(n_neighbors=k)
    score = cross_val_score(knn_classifier, X, Y, cv=10)
    knn_score.append(score.mean())

plt.figure(figsize=(8, 5))
plt.plot(range(1, 21), knn_score, color='red', marker='o')

# Add only score values as labels
for i in range(1, 21):
    plt.text(i, knn_score[i-1] + 0.001, f"{knn_score[i-1]:.3f}",
             ha='center', fontsize=8)

plt.xticks(range(1, 21))
plt.xlabel('Number of Neighbors (K)')
plt.ylabel('Score')
plt.title('K Neighbors Classifier scores for different K values')
plt.show()

best_k = range(1, 21)[knn_score.index(max(knn_score))]
best_score = max(knn_score)

plt.figure(figsize=(8, 5))
plt.plot(range(1, 21), knn_score, color='red', marker='o')
plt.text(best_k, best_score + 0.001, f"Best K={best_k}, Score={best_score:.3f}",
         ha='center', fontsize=9, color='blue')

plt.xticks(range(1, 21))
plt.xlabel('Number of Neighbors (K)')
plt.ylabel('Score')
plt.title('K Neighbors Classifier scores for different K values')
plt.show()

# Calculate accuracy scores for each model
from sklearn.metrics import accuracy_score

# Assuming dc_clf, rc_clf, svm_clf, and knn_classifier are your trained models
dc_accuracy = accuracy_score(Y_test, dc_clf.predict(X_test))
rf_accuracy = accuracy_score(Y_test, rc_clf.predict(X_test))
svm_accuracy = accuracy_score(Y_test, svm_clf.predict(X_test))

# Store accuracy results in a dictionary
accuracy_results = {
    "Decision Tree": dc_accuracy,
    "Random Forest": rf_accuracy,
    "SVM": svm_accuracy,
}

# Identify the best model
best_model = max(accuracy_results, key=accuracy_results.get)
print(f"\nBest Model: {best_model} with Accuracy: {accuracy_results[best_model]:.2%}")

